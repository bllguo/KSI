{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.load('data/training_data.npy', allow_pickle=True)\n",
    "test_data=np.load('data/test_data.npy', allow_pickle=True)\n",
    "val_data=np.load('data/val_data.npy', allow_pickle=True)\n",
    "word_to_ix=np.load('data/word_to_ix.npy', allow_pickle=True).item() # words (in notes) to index\n",
    "ix_to_word=np.load('data/ix_to_word.npy', allow_pickle=True).item() # index to words (in notes). not strictly needed for model\n",
    "wikivec=np.load('data/newwikivec.npy', allow_pickle=True) # wiki article embeddings (# codes with wiki articles, vocab size)\n",
    "wikivoc=np.load('data/wikivoc.npy', allow_pickle=True).item() # ICD-9 codes with wiki articles. not strictly needed for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wiki, n_vocab = wikivec.shape\n",
    "n_words = len(word_to_ix)\n",
    "n_embedding = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wikivec = torch.FloatTensor(wikivec).to(DEVICE) # wikivec is a model input for KSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(block):\n",
    "    block_size = len(block)\n",
    "    max_words = np.max([len(i[0]) for i in block])\n",
    "    mat = np.zeros((block_size, max_words), dtype=int)\n",
    "    for i in range(block_size):\n",
    "        for j in range(max_words):\n",
    "            try:\n",
    "                if block[i][0][j] in word_to_ix:\n",
    "                    mat[i,j] = word_to_ix[block[i][0][j]]\n",
    "            except IndexError:\n",
    "                pass\n",
    "    mat = torch.from_numpy(mat)\n",
    "    embeddings = torch.FloatTensor(np.array([x for _, x, _ in block]))\n",
    "    labels = torch.FloatTensor(np.array([y for _, _, y in block]))\n",
    "    return mat, embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, collate_fn=collate_fn, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_data, collate_fn=collate_fn, batch_size=test_batch_size)\n",
    "test_dataloader = DataLoader(test_data, collate_fn=collate_fn, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSI(nn.Module):\n",
    "    def __init__(self, n_ksi_embedding, n_vocab):\n",
    "        super().__init__()\n",
    "        self.ksi_embedding = nn.Linear(n_vocab, n_ksi_embedding)\n",
    "        self.ksi_attention = nn.Linear(n_ksi_embedding, n_ksi_embedding)\n",
    "        self.ksi_output = nn.Linear(n_ksi_embedding, 1)\n",
    "        \n",
    "    def forward_ksi(self, notevec, wikivec):\n",
    "        n = notevec.shape[0]\n",
    "        n_codes = wikivec.shape[0]\n",
    "        notevec = notevec.unsqueeze(1).expand(n, n_codes, -1)\n",
    "        wikivec = wikivec.unsqueeze(0)\n",
    "        \n",
    "        z = torch.mul(wikivec, notevec)\n",
    "        e = self.ksi_embedding(z)\n",
    "        attention_scores = torch.sigmoid(self.ksi_attention(e))\n",
    "        v = torch.mul(attention_scores, e)\n",
    "        s = self.ksi_output(v)\n",
    "        o = s.squeeze(2)\n",
    "        \n",
    "        return o\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_words, n_wiki, n_embedding, ksi=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ksi = ksi\n",
    "        self.word_embeddings = nn.Embedding(n_words+1, n_embedding)\n",
    "        self.dropout_embedding = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv1d(n_embedding, 100, 3)\n",
    "        self.conv2 = nn.Conv1d(n_embedding, 100, 4)\n",
    "        self.conv3 = nn.Conv1d(n_embedding, 100, 5)\n",
    "        self.output = nn.Linear(n_embedding*3, n_wiki)\n",
    "    \n",
    "    def forward(self, note, notevec=None, wikivec=None):\n",
    "        # batch_size, n = note.shape\n",
    "        embeddings = self.word_embeddings(note) # (batch_size, n, n_embedding)\n",
    "        embeddings = self.dropout_embedding(embeddings)\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, n_embedding, n)\n",
    "        \n",
    "        a1 = F.relu(self.conv1(embeddings))\n",
    "        a1 = F.max_pool1d(a1, a1.shape[2])\n",
    "        a2 = F.relu(self.conv2(embeddings))\n",
    "        a2 = F.max_pool1d(a2, a2.shape[2])\n",
    "        a3 = F.relu(self.conv3(embeddings))\n",
    "        a3 = F.max_pool1d(a3, a3.shape[2])\n",
    "        combined = torch.cat([a1, a2, a3], 1).squeeze(2)\n",
    "       \n",
    "        out = self.output(combined)\n",
    "        if self.ksi:\n",
    "            out += self.ksi.forward_ksi(notevec, wikivec)\n",
    "        \n",
    "        scores = torch.sigmoid(out)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimizer, wikivec=None):\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        note, embeddings, labels = data\n",
    "        note = note.to(DEVICE)\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        scores = model(note, embeddings, wikivec)\n",
    "        loss = loss_function(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "def test(model, dataloader, wikivec=None, threshold=0.5):\n",
    "    micro_f1 = []\n",
    "    macro_f1 = []\n",
    "    micro_auc = []\n",
    "    macro_auc = []\n",
    "    weights = []\n",
    "    for data in dataloader:\n",
    "        note, embeddings, labels = data\n",
    "        note = note.to(DEVICE)\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        out = model(note, embeddings, wikivec).cpu().detach().numpy()\n",
    "        pred = np.array(out > threshold, dtype=float)\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        \n",
    "        labeled_rows = np.sum(labels, axis=1) > 0 # exclude rows with no labels, which break sklearn metrics\n",
    "        filtered_labels = labels[labeled_rows].T\n",
    "        filtered_pred = pred[labeled_rows].T\n",
    "        filtered_scores = out[labeled_rows].T\n",
    "        \n",
    "        #TODO: recall @ k (metric used in the paper)\n",
    "        micro_f1.append(f1_score(filtered_labels, filtered_pred, average='micro'))\n",
    "        macro_f1.append(f1_score(filtered_labels, filtered_pred, average='macro'))\n",
    "        micro_auc.append(roc_auc_score(filtered_labels, filtered_scores, average='micro'))\n",
    "        macro_auc.append(roc_auc_score(filtered_labels, filtered_scores, average='macro'))\n",
    "        weights.append(len(data))\n",
    "    micro_f1 = np.average(micro_f1, weights=weights)\n",
    "    macro_f1 = np.average(macro_f1, weights=weights)\n",
    "    micro_auc = np.average(micro_auc, weights=weights)\n",
    "    macro_auc = np.average(macro_auc, weights=weights)\n",
    "    return None, micro_f1, macro_f1, micro_auc, macro_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = CNN(n_words, n_wiki, n_embedding)\n",
    "base_model = base_model.to(DEVICE)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(base_model.parameters())\n",
    "\n",
    "for epoch in range(1):\n",
    "    train(base_model, train_dataloader, loss_function, optimizer)\n",
    "    t_recall_at_k, t_micro_f1, t_macro_f1, t_micro_auc, t_macro_auc = test(base_model, train_dataloader)\n",
    "    v_recall_at_k, v_micro_f1, v_macro_f1, v_micro_auc, v_macro_auc = test(base_model, val_dataloader)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train Micro F1: {t_micro_f1:.4f}, Val Micro F1: {v_micro_f1:.4f}' +\n",
    "          f', Train Macro F1: {t_macro_f1:.4f}, Val Macro F1: {v_macro_f1:.4f}')\n",
    "    \n",
    "torch.save(base_model, 'CNN_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksi = KSI(n_embedding, n_vocab)\n",
    "ksi.to(DEVICE)\n",
    "model = CNN(n_words, n_wiki, n_embedding, ksi=ksi)\n",
    "model = model.to(DEVICE)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())  \n",
    "        \n",
    "for epoch in range(1):\n",
    "    train(model, train_dataloader, loss_function, optimizer, wikivec)\n",
    "    t_recall_at_k, t_micro_f1, t_macro_f1, t_micro_auc, t_macro_auc = test(model, train_dataloader, wikivec)\n",
    "    v_recall_at_k, v_micro_f1, v_macro_f1, v_micro_auc, v_macro_auc = test(model, val_dataloader, wikivec)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train Micro F1: {t_micro_f1:.4f}, Val Micro F1: {v_micro_f1:.4f}' +\n",
    "          f', Train Macro F1: {t_macro_f1:.4f}, Val Macro F1: {v_macro_f1:.4f}')\n",
    "    \n",
    "torch.save(model, 'KSI_CNN_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57e07979f6a7af2a0b0e861d549d9c40e5b4b1911b131063753718048dd868ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
